{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "HW2_2_Generation_2023088231_황주빈",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jubin0615/DL/blob/main/HW2_2_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"datasets==2.21.0\" protobuf==3.20.3 evaluate sacrebleu sentencepiece transformers[torch]"
      ],
      "metadata": {
        "id": "0p9ewxeslBnj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-04T02:08:53.233612Z",
          "iopub.execute_input": "2025-12-04T02:08:53.233895Z",
          "iopub.status.idle": "2025-12-04T02:10:08.269083Z",
          "shell.execute_reply.started": "2025-12-04T02:08:53.233874Z",
          "shell.execute_reply": "2025-12-04T02:10:08.268135Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers[torch] datasets evaluate sacrebleu sentencepiece"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-04T02:10:08.270666Z",
          "iopub.execute_input": "2025-12-04T02:10:08.270924Z",
          "iopub.status.idle": "2025-12-04T02:10:11.908367Z",
          "shell.execute_reply.started": "2025-12-04T02:10:08.270898Z",
          "shell.execute_reply": "2025-12-04T02:10:11.907262Z"
        },
        "id": "for2w5SsswsK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load english-korean sentence pairs\n",
        "# dataset = load_dataset(\"IWSLT/iwslt2017\", \"iwslt2017-ko-en\")\n",
        "dataset = load_dataset(\"IWSLT/iwslt2017\", \"iwslt2017-fr-en\", trust_remote_code=True)\n",
        "\n",
        "## Since the number of training data is too large, it will take a long time to train. So, let's just use a subset of training data\n",
        "# You can use any number instead of 5000. But all you have to do is, achieve higher score than 0.1 BLEU score.\n",
        "dataset['train'] = dataset['train'].select(range(5000))\n",
        "\n",
        "# Possible language pairs\n",
        "#'iwslt2017-en-it', 'iwslt2017-en-nl', 'iwslt2017-en-ro', 'iwslt2017-it-en', 'iwslt2017-it-nl',\n",
        "#'iwslt2017-it-ro', 'iwslt2017-nl-en', 'iwslt2017-nl-it', 'iwslt2017-nl-ro', 'iwslt2017-ro-en',\n",
        "#'iwslt2017-ro-it', 'iwslt2017-ro-nl', 'iwslt2017-ar-en', 'iwslt2017-de-en', 'iwslt2017-en-ar',\n",
        "#'iwslt2017-en-de', 'iwslt2017-en-fr', 'iwslt2017-en-ja', 'iwslt2017-en-ko', 'iwslt2017-en-zh',\n",
        "#'iwslt2017-fr-en', 'iwslt2017-ja-en', 'iwslt2017-ko-en', 'iwslt2017-zh-en'\n",
        "\n",
        "# If you plan to use the dataset that has only a train data, then execute the following, otherwise pass it\n",
        "# Split into train (70%), validation (15%), and test (15%)\n",
        "# train_test_split = dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
        "# validation_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Combine splits into a new DatasetDict\n",
        "# dataset = DatasetDict({\n",
        "#     'train': train_test_split['train'],\n",
        "#     'validation': validation_test_split['train'],\n",
        "#     'test': validation_test_split['test']\n",
        "# })\n",
        "\n",
        "#Do not change the below\n",
        "dataset['test'] = dataset['test'].select(range(100))\n",
        "dataset['validation'] = dataset['validation'].select(range(100))\n",
        "print(dataset)\n",
        "for i in dataset['validation']['translation'][:10]:\n",
        "    print(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "7UjmGZ6klica",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-04T02:10:11.909727Z",
          "iopub.execute_input": "2025-12-04T02:10:11.910551Z",
          "iopub.status.idle": "2025-12-04T02:10:19.955844Z",
          "shell.execute_reply.started": "2025-12-04T02:10:11.910522Z",
          "shell.execute_reply": "2025-12-04T02:10:19.955203Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration,\n",
        "    MBart50TokenizerFast,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.src_lang = \"fr_XX\"  # 입력: 프랑스어\n",
        "tokenizer.tgt_lang = \"en_XX\"  # 출력: 영어\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex['fr'] for ex in examples['translation']]\n",
        "    targets = [ex['en'] for ex in examples['translation']]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        text_target=targets,\n",
        "        max_length=64,\n",
        "        truncation=True\n",
        "    )\n",
        "    return model_inputs\n",
        "\n",
        "# 데이터셋에 적용\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # 디코딩\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # 공백 정리\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": result[\"score\"]/100}\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./mbart_fr_en\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=4,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"en_XX\"]\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-04T02:10:19.957202Z",
          "iopub.execute_input": "2025-12-04T02:10:19.95749Z",
          "iopub.status.idle": "2025-12-04T02:34:59.301455Z",
          "shell.execute_reply.started": "2025-12-04T02:10:19.95747Z",
          "shell.execute_reply": "2025-12-04T02:34:59.300751Z"
        },
        "id": "NqkIgJIlswsK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = trainer.predict(tokenized_datasets[\"test\"])\n",
        "print(f\"BLEU score on test data is {test_results.metrics['test_bleu']}\")"
      ],
      "metadata": {
        "id": "TnA7CP-6VLkL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-04T02:34:59.302349Z",
          "iopub.execute_input": "2025-12-04T02:34:59.302619Z",
          "iopub.status.idle": "2025-12-04T02:35:23.430228Z",
          "shell.execute_reply.started": "2025-12-04T02:34:59.302595Z",
          "shell.execute_reply": "2025-12-04T02:35:23.429627Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}