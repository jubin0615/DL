{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKelMv2k5gNx",
        "outputId": "48488140-0910-4597-f956-c932876864cb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file \"hw1_data.tsv\"\n",
        "file = open(\"/content/drive/MyDrive/hw1_data.tsv\",'r')\n",
        "ori_data = file.read().strip().split(\"\\n\")\n",
        "data = []\n",
        "for item in ori_data:\n",
        "  item = item.split(\"\\t\")\n",
        "  if len(item[0]) >0:\n",
        "    data.append((item[0],item[1]))\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObQrvnbB8hPZ",
        "outputId": "f079b3b6-c34b-4631-faca-fb0d595eef5f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSiVjnSKBlcK",
        "outputId": "83c917bb-b81a-4cad-cc55-dc53fa4e1871"
      },
      "source": [
        "### Training data pre-processing\n",
        "texts, labels = [], []\n",
        "label2idx = {\"0\":0, \"1\":1}\n",
        "for i, item in enumerate(data):\n",
        "  text = item[0]\n",
        "\n",
        "  ## Preprocessing (if you want to add, please add more)\n",
        "  ################################################################################\n",
        "  text = text.replace(\"  \",\" \") ## Replace double space\n",
        "  text = text.replace(\",\", \"\") ## Replace comma to \"\"\n",
        "  text = text.lower()  ## Lower cases\n",
        "  ################################################################################\n",
        "  label = label2idx[item[1]]\n",
        "\n",
        "  texts.append(text)\n",
        "  labels.append(label)\n",
        "\n",
        "print(\"*\"*50)\n",
        "print(\"Total number of datasets\")\n",
        "print(len(texts))\n",
        "print(len(labels))\n",
        "print(\"*\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Total number of datasets\n",
            "300\n",
            "300\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Split into train/dev/test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "### Write a code for collecting samples for each class\n",
        "################################################################################\n",
        "pos,neg = [], []\n",
        "train_texts, dev_texts, test_texts, train_labels, dev_labels, test_labels = [], [], [], [], [], []\n",
        "for a,b in zip(texts,labels):\n",
        "  if b == 1:\n",
        "    pos.append((a,b))\n",
        "  else:\n",
        "    neg.append((a,b))\n",
        "\n",
        "for a,b in pos[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "for a,b in neg[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "\n",
        "for a,b in pos[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "for a,b in neg[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "\n",
        "for a,b in pos[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "for a,b in neg[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "################################################################################\n",
        "\n",
        "\n",
        "tmp = list(zip(train_texts,train_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "train_texts, train_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(dev_texts,dev_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "dev_texts, dev_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(test_texts,test_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "test_texts, test_labels = zip(*tmp)\n",
        "\n",
        "print(\"Train Dataset Examples\")\n",
        "print(train_texts[:3])\n",
        "print(train_labels[:3])\n",
        "print(\"*\"*50)\n",
        "print(train_labels)\n",
        "print(dev_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZyzodaGovkH",
        "outputId": "4a946418-fa80-45a7-cc57-cdf732f3b061"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Examples\n",
            "(\"are worth the price of admission ... if `` gory mayhem '' is your idea of a good time \", 'was produced by jerry bruckheimer and directed by joel schumacher  and reflects the worst of their shallow styles : wildly overproduced  inadequately motivated every step of the way and demographically targeted to please every one ( and no one ) ', 'the whole affair  true story or not  feels incredibly hokey ... ')\n",
            "(1, 0, 0)\n",
            "**************************************************\n",
            "(1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0)\n",
            "(0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4wmL0jRBw2_",
        "outputId": "d5b9e393-430c-4a71-ea2a-7ca852b698a1"
      },
      "source": [
        "## Construct a vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "all_words = []\n",
        "for item in train_texts:\n",
        "  all_words += item.split()\n",
        "for item in dev_texts:\n",
        "  all_words += item.split()\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(all_words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {'<pad>':0, \"<unk>\":1}\n",
        "vocab_to_int.update({word: ii for ii, word in enumerate(vocab,2)})\n",
        "print(vocab_to_int)\n",
        "print(len(vocab_to_int))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, '<unk>': 1, 'the': 2, 'and': 3, 'a': 4, 'to': 5, '.': 6, 'of': 7, 'it': 8, 'in': 9, 'is': 10, 'that': 11, \"'s\": 12, 'as': 13, 'for': 14, 'with': 15, 'you': 16, 'film': 17, 'one': 18, 'movie': 19, 'by': 20, 'this': 21, 'its': 22, '--': 23, '...': 24, 'have': 25, 'good': 26, 'story': 27, 'so': 28, 'if': 29, 'your': 30, 'time': 31, 'no': 32, 'not': 33, \"n't\": 34, 'most': 35, 'more': 36, 'are': 37, 'way': 38, '(': 39, ')': 40, 'about': 41, 'on': 42, 'be': 43, 'but': 44, 'too': 45, 'very': 46, 'i': 47, 'an': 48, 'little': 49, 'takes': 50, 'own': 51, '``': 52, 'worst': 53, 'or': 54, 'from': 55, 'some': 56, 'picture': 57, 'could': 58, 'when': 59, 'just': 60, 'at': 61, 'can': 62, 'his': 63, 'comedy': 64, \"''\": 65, 'their': 66, 'every': 67, 'surprisingly': 68, 'better': 69, 'does': 70, 'watching': 71, 'moments': 72, 'into': 73, 'all': 74, 'hate': 75, 'life': 76, 'many': 77, 'old': 78, 'like': 79, 'recent': 80, 'what': 81, 'completely': 82, 'men': 83, 'work': 84, 'worth': 85, 'was': 86, ':': 87, 'wildly': 88, 'step': 89, 'whole': 90, 'feels': 91, 'ride': 92, 'even': 93, 'sensitive': 94, 'look': 95, 'point': 96, 'small': 97, 'considerable': 98, 'aplomb': 99, 'will': 100, 'laughs': 101, 'may': 102, 'unfortunately': 103, 'superb': 104, 'than': 105, 'mess': 106, 'new': 107, 'plot': 108, 'awful': 109, 'part': 110, 'late': 111, 'poignant': 112, 'talented': 113, 'tribute': 114, 'two': 115, 'being': 116, 'again': 117, 'heart': 118, 'uses': 119, 'sensational': 120, 'crime': 121, 'effects': 122, 'make': 123, 'well': 124, 'seen': 125, 'noir': 126, 'who': 127, 'has': 128, 'line': 129, 'charm': 130, 'which': 131, 'he': 132, 'want': 133, 'dumb': 134, 'fun': 135, 'through': 136, 'once': 137, 'horror': 138, 'itself': 139, 'derivative': 140, 'seems': 141, 'kind': 142, 'dialogue': 143, 'much': 144, 'romance': 145, 'serves': 146, 'despite': 147, 'amounts': 148, 'rock': 149, 'only': 150, 'fear': 151, 'epic': 152, 'digital': 153, 'suspense': 154, 'depth': 155, 'put': 156, 'almost': 157, 'any': 158, 'year': 159, 'sometimes': 160, 'cuteness': 161, 'makes': 162, 'conviction': 163, 'would': 164, 'justify': 165, 'theatrical': 166, 'simulation': 167, 'death': 168, 'camp': 169, 'auschwitz': 170, 'ii-birkenau': 171, 'sketchy': 172, 'together': 173, 'flaws': 174, 'young': 175, 'out': 176, 'think': 177, 'enough': 178, 'how': 179, 'family': 180, 'watch': 181, 'history': 182, 'silly': 183, 'outrageous': 184, 'special': 185, 'best': 186, 'beautiful': 187, 'pay': 188, 'scene': 189, 'after': 190, 'minutes': 191, 'jason': 192, 'attractive': 193, 'cinematic': 194, 'price': 195, 'admission': 196, 'gory': 197, 'mayhem': 198, 'idea': 199, 'produced': 200, 'jerry': 201, 'bruckheimer': 202, 'directed': 203, 'joel': 204, 'schumacher': 205, 'reflects': 206, 'shallow': 207, 'styles': 208, 'overproduced': 209, 'inadequately': 210, 'motivated': 211, 'demographically': 212, 'targeted': 213, 'please': 214, 'affair': 215, 'true': 216, 'incredibly': 217, 'hokey': 218, 'dim': 219, 'enjoy': 220, 'traditionally': 221, 'structured': 222, 'everything': 223, 'girls': 224, 'ca': 225, 'swim': 226, 'passages': 227, 'observation': 228, 'secondhand': 229, 'familiar': 230, 'showing': 231, 'honest': 232, 'emotions': 233, 'predictable': 234, 'tides': 235, 'smeary': 236, 'blurry': 237, 'distraction': 238, 'evanescent': 239, 'seamless': 240, 'sumptuous': 241, 'stream': 242, 'enables': 243, 'shafer': 244, 'navigate': 245, 'spaces': 246, 'both': 247, 'large': 248, 'breathless': 249, 'anticipation': 250, 'working': 251, 'script': 252, 'co-written': 253, 'gianni': 254, 'romoli': 255, 'ozpetek': 256, 'avoids': 257, 'pitfalls': 258, \"'d\": 259, 'expect': 260, 'such': 261, 'potentially': 262, 'sudsy': 263, 'set-up': 264, 'bogus': 265, 'setup': 266, 'easy': 267, 'borders': 268, 'facile': 269, 'home': 270, 'leave': 271, 'wanting': 272, 'mention': 273, 'leaving': 274, 'smile': 275, 'face': 276, 'confessions': 277, 'straightforward': 278, 'bio': 279, 'fascinating': 280, 'byways': 281, 'esther': 282, 'kahn': 283, 'unusual': 284, 'also': 285, 'irritating': 286, 'technically': 287, 'ludicrous': 288, 'conceptions': 289, 'thoroughly': 290, 'thanks': 291, 'lau': 292, 'stylish': 293, 'exercise': 294, 'arrive': 295, 'early': 296, 'stay': 297, 'significantly': 298, 'usual': 299, 'leavened': 300, 'entirely': 301, 'persuasive': 302, 'give': 303, 'exposure': 304, 'performers': 305, 'warm': 306, 'water': 307, 'under': 308, 'red': 309, 'bridge': 310, 'celebration': 311, 'feminine': 312, 'energy': 313, 'power': 314, 'women': 315, 'heal': 316, 'shot': 317, 'artful': 318, 'watery': 319, 'tones': 320, 'blue': 321, 'green': 322, 'brown': 323, 'either': 324, 'used': 325, 'my': 326, 'hours': 327, 'john': 328, 'malkovich': 329, 'vicious': 330, 'absurd': 331, 'pan-american': 332, 'genuine': 333, 'insight': 334, 'urban': 335, 'real-life': 336, '19th-century': 337, 'metaphor': 338, 'amazingly': 339, 'lame': 340, 'rich': 341, 'full': 342, 'her': 343, 'charmless': 344, 'guns': 345, 'cheatfully': 346, 'filmed': 347, 'martial': 348, 'arts': 349, 'disintegrating': 350, 'bloodsucker': 351, 'computer': 352, 'jagged': 353, 'camera': 354, 'moves': 355, 'retains': 356, 'ambiguities': 357, 'things': 358, 'we': 359, \"'ve\": 360, 'before': 361, 'unorthodox': 362, 'organized': 363, 'includes': 364, 'strangest': 365, 'degraded': 366, 'handheld': 367, 'blair': 368, 'witch': 369, 'video-cam': 370, 'footage': 371, 'hugh': 372, 'grant': 373, 'valuable': 374, 'messages': 375, 'tedious': 376, 'norwegian': 377, 'offering': 378, 'somehow': 379, 'snagged': 380, 'oscar': 381, 'nomination': 382, 'second': 383, 'fiddle': 384, 'because': 385, 'acts': 386, 'goofy': 387, 'hold': 388, 'grips': 389, 'hard': 390, 'guys': 391, 'desperately': 392, 'quentin': 393, 'tarantino': 394, 'they': 395, 'grow': 396, 'up': 397, 'take': 398, 'care': 399, 'nicely': 400, 'performed': 401, 'quintet': 402, 'actresses': 403, 'acted': 404, 'diane': 405, 'lane': 406, 'richard': 407, 'gere': 408, 'dentist': 409, 'waiting': 410, 'room': 411, 'understand': 412, 'difference': 413, 'between': 414, 'plain': 415, 'halfway': 416, 'beginning': 417, 'manages': 418, 'infuse': 419, 'rocky': 420, 'path': 421, 'sibling': 422, 'reconciliation': 423, 'flashes': 424, 'warmth': 425, 'gentle': 426, 'humor': 427, 'laughable': 428, 'compulsively': 429, 'watchable': 430, 'chances': 431, 'bold': 432, 'studio': 433, 'standards': 434, 'scant': 435, 'adventurous': 436, 'indian': 437, 'filmmakers': 438, 'toward': 439, 'crossover': 440, 'nonethnic': 441, 'markets': 442, 'folly': 443, 'superficiality': 444, 'comic': 445, 'gem': 446, 'delightful': 447, 'positive': 448, 'change': 449, 'tone': 450, 'here': 451, 'recharged': 452, 'him': 453, 'kinetic': 454, 'teeming': 455, 'cranky': 456, 'adults': 457, 'rediscover': 458, 'quivering': 459, 'kid': 460, 'inside': 461, 'elevated': 462, 'ultra-cheesy': 463, 'spare': 464, 'wildlife': 465, 'frida': 466, 'different': 467, 'hollywood': 468, 'bewilderingly': 469, 'brilliant': 470, 'entertaining': 471, 'pays': 472, 'earnest': 473, 'homage': 474, 'turntablists': 475, 'beat': 476, 'jugglers': 477, 'schoolers': 478, 'current': 479, 'innovators': 480, 'essentially': 481, 'collection': 482, 'bits': 483, 'résumé': 484, 'loaded': 485, 'credits': 486, 'girl': 487, 'bar': 488, '#': 489, '3': 490, 'auto-critique': 491, 'clumsiness': 492, 'damning': 493, 'censure': 494, 'disappointingly': 495, 'thin': 496, 'slice': 497, 'lower-class': 498, 'london': 499, ';': 500, 'title': 501, 'extremely': 502, 'unpleasant': 503, 'carnage': 504, 'smooth': 505, 'professional': 506, 'add': 507, 'beyond': 508, 'dark': 509, 'visions': 510, 'already': 511, 'relayed': 512, 'predecessors': 513, 'fighting': 514, 'skills': 515, 'steven': 516, 'seagal': 517, 'speak': 518, 'while': 519, 'forces': 520, 'ponder': 521, 'anew': 522, 'thing': 523, 'dot': 524, 'com': 525, 'breezy': 526, 'distracted': 527, 'rhythms': 528, 'value': 529, 'respect': 530, 'term': 531, 'cinema': 532, 'ugly': 533, 'shabby': 534, 'photography': 535, 'handsome': 536, 'unfulfilling': 537, 'drama': 538, 'merit': 539, '103-minute': 540, 'length': 541, 'added': 542, 'resonance': 543, 'bode': 544, 'rest': 545, 'admittedly': 546, 'middling': 547, 'yes': 548, 'snail-like': 549, 'pacing': 550, 'slick': 551, 'manufactured': 552, 'claim': 553, 'street': 554, 'credibility': 555, 'unassuming': 556, 'subordinate': 557, 'build': 558, 'robots': 559, 'haul': 560, \"'em\": 561, 'theater': 562, 'show': 563, 'mystery': 564, 'science': 565, 'theatre': 566, '3000': 567, 'certainly': 568, 'going': 569, 'go': 570, 'down': 571, 'killer': 572, 'website': 573, 'other': 574, 'caruso': 575, 'descends': 576, 'sub-tarantino': 577, 'sure': 578, 'salton': 579, 'sea': 580, 'works': 581, 'should': 582, 'keeping': 583, 'tight': 584, 'nasty': 585, 'there': 586, 'something': 587, 'artist': 588, '90-plus': 589, 'years': 590, 'taking': 591, 'effort': 592, 'share': 593, 'impressions': 594, 'loss': 595, 'art': 596, 'us': 597, 'lacking': 598, 'surprise': 599, 'consistent': 600, 'emotional': 601, 'collapse': 602, 'extraordinary': 603, 'faith': 604, 'digital-effects-heavy': 605, 'supposed': 606, 'family-friendly': 607, 'disney': 608, 'ransacks': 609, 'archives': 610, 'quick-buck': 611, 'sequel': 612, 'grievous': 613, 'character': 614, 'dramas': 615, 'never': 616, 'reach': 617, 'satisfying': 618, 'conclusions': 619, 'well-written': 620, 'well-acted': 621, 'gender-bending': 622, 'generally': 623, 'quite': 624, 'funny': 625, 'altogether': 626, 'slight': 627, 'called': 628, 'masterpiece': 629, 'liked': 630, 'had': 631, 'gone': 632, 'further': 633, 'trouble': 634, 'day': 635, 'preliminary': 636, 'notes': 637, 'science-fiction': 638, 'fragmentary': 639, 'narrative': 640, 'style': 641, 'piecing': 642, 'frustrating': 643, 'difficult': 644, 'clumsy': 645, 'heavy-handed': 646, 'phoney-feeling': 647, 'sentiment': 648, 'yet': 649, 'grating': 650, 'showcase': 651, 'welcome': 652, 'relief': 653, 'challenges': 654, 'poses': 655, 'forgive': 656, 'overlong': 657, 'bombastic': 658, 'interesting': 659, 'cobbled': 660, 'largely': 661, 'flat': 662, 'uncreative': 663, 'eats': 664, 'meddles': 665, 'argues': 666, 'kibbitzes': 667, 'fights': 668, 'delivers': 669, 'promises': 670, 'wild': 671, 'ensues': 672, 'brash': 673, 'set': 674, 'conquer': 675, 'online': 676, 'world': 677, 'laptops': 678, 'cell': 679, 'phones': 680, 'business': 681, 'plans': 682, 'tasteful': 683, 'roll': 684, 'plodding': 685, 'vainly': 686, 'absolutely': 687, 'ridiculous': 688, 'respectable': 689, 'hammily': 690, 'trying': 691, 'grab': 692, 'lump': 693, 'play-doh': 694, 'harder': 695, 'liman': 696, 'tries': 697, 'squeeze': 698, 'told': 699, 'scattered': 700, 'fashion': 701, 'compelling': 702, 'tear': 703, 'eyes': 704, 'away': 705, 'images': 706, 'long': 707, 'read': 708, 'subtitles': 709, 'overbearing': 710, 'over-the-top': 711, 'lively': 712, 'engaging': 713, 'examination': 714, 'similar': 715, 'obsessions': 716, 'dominate': 717, 'proves': 718, 'lovely': 719, 'trifle': 720, 'love': 721, 'promise': 722, 'filmmaking': 723, 'supremely': 724, 'unfunny': 725, 'unentertaining': 726, 'middle-age': 727, 'workable': 728, 'primer': 729, 'region': 730, 'terrific': 731, '10th-grade': 732, 'learning': 733, 'tool': 734, 'ingenious': 735, 'nicks': 736, 'steinberg': 737, 'match': 738, 'creations': 739, 'pure': 740, 'venality': 741, 'giving': 742, 'college': 743, 'try': 744, 'reveals': 745, 'important': 746, 'our': 747, 'talents': 748, 'service': 749, 'others': 750, 'hawaiian': 751, 'shirt': 752, 'adaptation': 753, 'do': 754, 'concert': 755, 'imax': 756, 'short': 757, 'easily': 758, 'wait': 759, 'per': 760, 'view': 761, 'dollar': 762, 'unpretentious': 763, 'charming': 764, 'quirky': 765, 'original': 766, 'black': 767, 'ii': 768, 'achieves': 769, 'ultimate': 770, 'insignificance': 771, 'sci-fi': 772, 'spectacle': 773, 'whiffle-ball': 774, 'mid-to-low': 775, 'budget': 776, 'betrayed': 777, 'shoddy': 778, 'makeup': 779, 'bring': 780, 'tissues': 781, 'uneven': 782, 'direction': 783, 'fluid': 784, 'no-nonsense': 785, 'authority': 786, 'performances': 787, 'harris': 788, 'phifer': 789, 'cam': 790, '`': 791, 'ron': 792, 'seal': 793, 'deal': 794, 'lika': 795, 'da': 796, 'wide-awake': 797, 'conclusive': 798, 'answers': 799, 'big': 800, 'excuse': 801, 'play': 802, 'lewd': 803, 'another': 804, 'runs': 805, 'mere': 806, '84': 807, 'glance': 808, 'believe': 809, 'actually': 810, 'backseat': 811, 'provide': 812, 'keenest': 813, 'pleasures': 814, 'underscore': 815, 'importance': 816, 'tradition': 817, 'familial': 818, 'community': 819, 'elvira': 820, 'fans': 821, 'hardly': 822, 'ask': 823, 'irritates': 824, 'effective': 825, 'stick': 826, 'those': 827, 'so-so': 828, 'films': 829, 'been': 830, 'willing': 831, 'champion': 832, 'fallibility': 833, 'human': 834, 'audacious': 835, 'movies': 836, 'suck': 837, 'passable': 838, 'date': 839, 'able': 840, 'hit': 841, '15-year': 842, \"'re\": 843, 'over': 844, '100': 845, 'infectiously': 846, 'unpredictable': 847, 'color': 848, 'rather': 849, 'laugh': 850, 'maybe': 851, 'twice': 852, 'forgotten': 853, 'get': 854, 'back': 855, 'car': 856, 'parking': 857, 'lot': 858, 'pleasant': 859, 'oozing': 860, 'bon': 861, 'bons': 862, 'pokes': 863, 'provokes': 864, 'expressionistic': 865, 'license': 866, 'remarkable': 867, 'procession': 868, 'sweeping': 869, 'pictures': 870, 'reinvigorated': 871, 'genre': 872, 'addition': 873, 'sporting': 874, 'titles': 875, 'dog': 876, 'spears': 877, \"'\": 878, 'music': 879, 'videos': 880, 'content': 881, 'except': 882, 'goes': 883, 'least': 884, '90': 885, 'worse': 886, 'see': 887, 'harsh': 888, 'piece': 889, 'storytelling': 890, 'problem': 891, 'whether': 892, 'these': 893, 'ambitions': 894, 'laudable': 895, 'themselves': 896, 'involved': 897, 'save': 898, 'dash': 899, 'shows': 900, 'slightest': 901, 'aptitude': 902, 'acting': 903, 'lost': 904, 'translation': 905, 'inane': 906, 'brings': 907, 'proper': 908, 'role': 909, 'bourne': 910, 'starts': 911, 'off': 912, 'bad': 913, 'feel': 914, 'running': 915, 'screaming': 916, 'classic': 917, 'casts': 918, 'actors': 919, 'magnificent': 920, 'landscape': 921, 'create': 922, 'feature': 923, 'wickedly': 924, 'hopeless': 925, 'well-thought': 926, 'stunts': 927, 'final': 928, 'realistic': 929, 'portrayal': 930, 'woman': 931, 'great': 932, 'generosity': 933, 'diplomacy': 934}\n",
            "935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def encode_sentence(sentence):\n",
        "    max_length = 50\n",
        "    input_ids = []\n",
        "    for item in sentence.split():\n",
        "      if item in vocab_to_int:\n",
        "        input_ids.append(vocab_to_int[item])\n",
        "      else:\n",
        "        input_ids.append(vocab_to_int['<unk>'])\n",
        "\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids += [vocab_to_int['<pad>']] * padding_length\n",
        "    return np.array(input_ids)\n",
        "\n",
        "def encode_label(label):\n",
        "    return np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Train Dataset Encode Examples\")\n",
        "for a,b in zip(train_texts[:3],train_labels[:3]):\n",
        "  print(encode_sentence(a),b)"
      ],
      "metadata": {
        "id": "GJ0RsEBgqidq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a013313-b542-447c-85b6-55f46de843d2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Encode Examples\n",
            "[ 37  85   2 195   7 196  24  29  52 197 198  65  10  30 199   7   4  26\n",
            "  31   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n",
            "[ 86 200  20 201 202   3 203  20 204 205   3 206   2  53   7  66 207 208\n",
            "  87  88 209 210 211  67  89   7   2  38   3 212 213   5 214  67  18  39\n",
            "   3  32  18  40   0   0   0   0   0   0   0   0   0   0] 0\n",
            "[  2  90 215 216  27  54  33  91 217 218  24   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.embedding_weights = np.random.rand(vocab_size, embedding_dim) # Embedding weights\n",
        "        # limit = np.sqrt(6 / (embedding_dim + hidden_size))\n",
        "        # self.W1 = np.random.uniform(-limit, limit, size=(embedding_dim, hidden_size))\n",
        "        # limit = np.sqrt(6/ (hidden_size+output_size))\n",
        "        # self.W2 = np.random.uniform(-limit, limit, size=(hidden_size, output_size))\n",
        "        self.W1 = np.random.rand(embedding_dim, hidden_size)   # Input to hidden weights\n",
        "        self.W2 = np.random.rand(hidden_size, output_size) # Hidden to output weights\n",
        "        self.b1 = np.zeros((1, hidden_size))  # Hidden layer biases\n",
        "        self.b2 = np.zeros((1, output_size))  # Output layer biases\n",
        "        self.X = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass\n",
        "        embedded_x = self.embedding_weights[X]\n",
        "        self.sentence_vec = np.mean(embedded_x, axis=0, keepdims=True)\n",
        "\n",
        "        z1 = np.dot(self.sentence_vec, self.W1) + self.b1\n",
        "        self.h = self.sigmoid(z1)\n",
        "\n",
        "        z2 = np.dot(self.h, self.W2) + self.b2\n",
        "        output = self.sigmoid(z2)\n",
        "        return output\n",
        "\n",
        "    def compute_loss(self, y, output):\n",
        "        # Compute binary cross-entropy loss\n",
        "        epsilon = 1e-9\n",
        "        loss = -np.sum(y * np.log(output + epsilon) + (1 - y) * np.log(1 - output + epsilon))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.01):\n",
        "      delta2 = output - y\n",
        "      dW2 = np.dot(self.h.T, delta2)\n",
        "      db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "      delta1 = np.dot(delta2, self.W2.T) * self.sigmoid_derivative(self.h)\n",
        "      dW1 = np.dot(self.sentence_vec.T, delta1)\n",
        "      db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "      self.W1 -= learning_rate * dW1\n",
        "      self.b1 -= learning_rate * db1\n",
        "      self.W2 -= learning_rate * dW2\n",
        "      self.b2 -= learning_rate * db2"
      ],
      "metadata": {
        "id": "-76p4FBbAqLp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def train(X, y, learning_rate=0.01):\n",
        "    output = nn.forward(X)  # Forward pass\n",
        "    loss = nn.compute_loss(y, output)  # Compute loss\n",
        "    nn.backward(X, y, output, learning_rate)  # Backward pass\n",
        "    return loss\n",
        "\n",
        "def predict(x):\n",
        "    output = nn.forward(x)\n",
        "    return output, (output > 0.5).astype(int)  # Binary classification\n",
        "\n",
        "# Initialize the neural network\n",
        "vocab_size = len(vocab_to_int)  # Number of unique words in vocab\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "hidden_size = 80  # Number of neurons in the hidden layer\n",
        "output_size = 1  # One output (binary classification)\n",
        "learning_rate = 0.01\n",
        "nn = SimpleNN(vocab_size, embedding_dim, hidden_size, output_size)\n",
        "\n",
        "true, pred = [], []\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    true, pred = [], []  # Reset true and predicted labels for each epoch\n",
        "    for x, y in zip(train_texts, train_labels):\n",
        "        x = encode_sentence(x)  # Encode sentence as word indices\n",
        "        train_loss += train(x, y, learning_rate)  # Train on the current sample\n",
        "        _, prediction = predict(x)  # obtain prediction\n",
        "        true.append(y)  # Append true label\n",
        "        pred.append(prediction[0][0])  # Append predicted label (extract scalar)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_acc = accuracy_score(true, pred) * 100.0\n",
        "    train_loss /= len(train_texts)  # Average training loss\n",
        "\n",
        "    # Evaluate on dev set\n",
        "    dev_true, dev_pred = [], []\n",
        "    dev_loss = 0.0\n",
        "    for x, y in zip(dev_texts, dev_labels):\n",
        "        x = encode_sentence(x)\n",
        "\n",
        "        output, prediction = predict(x)\n",
        "        dev_loss += nn.compute_loss(y,output)\n",
        "        dev_true.append(y)\n",
        "        dev_pred.append(prediction[0][0])\n",
        "    dev_loss /= len(dev_texts)\n",
        "    dev_acc = accuracy_score(dev_true, dev_pred) * 100.0\n",
        "    print(f'{epoch} epoch, train_loss = {train_loss:.4f}, train_acc: {train_acc:.2f}%, eval_loss: {dev_loss:.4f}, eval_acc: {dev_acc:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBBBnxduXd_u",
        "outputId": "9a89506f-fa3f-4584-cbe0-b4fa8bea47aa"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 epoch, train_loss = 6.7795, train_acc: 53.00%, eval_loss: 0.6944, eval_acc: 50.00%\n",
            "1 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "2 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "3 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "4 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "5 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "6 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "7 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "8 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "9 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "10 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "11 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "12 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "13 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "14 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "15 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "16 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "17 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "18 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "19 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "20 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "21 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "22 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "23 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "24 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "25 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "26 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "27 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "28 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "29 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "30 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "31 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "32 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "33 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "34 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "35 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "36 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "37 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "38 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "39 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "40 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "41 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "42 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "43 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "44 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "45 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "46 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "47 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "48 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "49 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "50 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "51 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "52 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "53 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "54 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "55 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "56 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "57 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "58 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "59 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "60 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "61 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "62 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "63 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "64 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "65 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "66 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "67 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "68 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "69 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "70 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "71 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "72 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "73 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "74 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "75 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "76 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "77 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "78 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "79 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "80 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "81 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "82 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "83 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "84 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "85 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "86 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "87 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "88 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "89 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "90 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "91 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "92 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "93 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "94 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "95 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "96 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "97 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "98 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n",
            "99 epoch, train_loss = 0.7829, train_acc: 80.00%, eval_loss: 0.6989, eval_acc: 50.00%\n"
          ]
        }
      ]
    }
  ]
}