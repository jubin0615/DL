{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKelMv2k5gNx",
        "outputId": "49dc662c-0f6f-44d3-fe9c-da8db6935718"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file \"hw1_data.tsv\"\n",
        "file = open(\"/content/drive/MyDrive/hw1_data.tsv\",'r')\n",
        "ori_data = file.read().strip().split(\"\\n\")\n",
        "data = []\n",
        "for item in ori_data:\n",
        "  item = item.split(\"\\t\")\n",
        "  if len(item[0]) >0:\n",
        "    data.append((item[0],item[1]))\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObQrvnbB8hPZ",
        "outputId": "4b8f49f0-417e-4b9e-864a-4eb75714d938"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSiVjnSKBlcK",
        "outputId": "3e2ba5ef-1e77-4b59-fa96-91a30f510eba"
      },
      "source": [
        "### Training data pre-processing\n",
        "texts, labels = [], []\n",
        "label2idx = {\"0\":0, \"1\":1}\n",
        "for i, item in enumerate(data):\n",
        "  text = item[0]\n",
        "\n",
        "  ## Preprocessing (if you want to add, please add more)\n",
        "  ################################################################################\n",
        "  text = text.replace(\"  \",\" \") ## Replace double space\n",
        "  text = text.replace(\",\", \"\") ## Replace comma to \"\"\n",
        "  text = text.lower()  ## Lower cases\n",
        "  ################################################################################\n",
        "  label = label2idx[item[1]]\n",
        "\n",
        "  texts.append(text)\n",
        "  labels.append(label)\n",
        "\n",
        "print(\"*\"*50)\n",
        "print(\"Total number of datasets\")\n",
        "print(len(texts))\n",
        "print(len(labels))\n",
        "print(\"*\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Total number of datasets\n",
            "300\n",
            "300\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Split into train/dev/test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "### Write a code for collecting samples for each class\n",
        "################################################################################\n",
        "pos,neg = [], []\n",
        "train_texts, dev_texts, test_texts, train_labels, dev_labels, test_labels = [], [], [], [], [], []\n",
        "for a,b in zip(texts,labels):\n",
        "  if b == 1:\n",
        "    pos.append((a,b))\n",
        "  else:\n",
        "    neg.append((a,b))\n",
        "\n",
        "for a,b in pos[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "for a,b in neg[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "\n",
        "for a,b in pos[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "for a,b in neg[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "\n",
        "for a,b in pos[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "for a,b in neg[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "################################################################################\n",
        "\n",
        "\n",
        "tmp = list(zip(train_texts,train_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "train_texts, train_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(dev_texts,dev_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "dev_texts, dev_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(test_texts,test_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "test_texts, test_labels = zip(*tmp)\n",
        "\n",
        "print(\"Train Dataset Examples\")\n",
        "print(train_texts[:3])\n",
        "print(train_labels[:3])\n",
        "print(\"*\"*50)\n",
        "print(train_labels)\n",
        "print(dev_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZyzodaGovkH",
        "outputId": "fbd5d56f-63f5-49cf-87ed-72e9176fabaa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Examples\n",
            "(\"it 's smooth and professional \", 'an admittedly middling film ', 'takes hold and grips hard ')\n",
            "(1, 1, 1)\n",
            "**************************************************\n",
            "(1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0)\n",
            "(0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4wmL0jRBw2_",
        "outputId": "e86e8eee-aea2-4325-d28c-34cce0203626"
      },
      "source": [
        "## Construct a vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "all_words = []\n",
        "for item in train_texts:\n",
        "  all_words += item.split()\n",
        "for item in dev_texts:\n",
        "  all_words += item.split()\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(all_words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {'<pad>':0, \"<unk>\":1}\n",
        "vocab_to_int.update({word: ii for ii, word in enumerate(vocab,2)})\n",
        "print(vocab_to_int)\n",
        "print(len(vocab_to_int))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, '<unk>': 1, 'the': 2, 'and': 3, 'a': 4, 'to': 5, '.': 6, 'of': 7, 'it': 8, 'in': 9, 'that': 10, 'is': 11, \"'s\": 12, 'as': 13, 'for': 14, 'film': 15, 'you': 16, 'with': 17, 'one': 18, 'movie': 19, 'by': 20, 'this': 21, 'its': 22, '--': 23, '...': 24, 'have': 25, 'so': 26, 'story': 27, 'good': 28, 'most': 29, 'more': 30, 'not': 31, 'your': 32, \"n't\": 33, 'no': 34, 'time': 35, 'if': 36, 'an': 37, 'takes': 38, 'little': 39, 'on': 40, 'be': 41, 'but': 42, 'i': 43, 'too': 44, 'about': 45, 'way': 46, 'are': 47, '(': 48, ')': 49, 'own': 50, 'very': 51, 'some': 52, 'can': 53, '``': 54, 'from': 55, 'could': 56, 'worst': 57, 'when': 58, 'picture': 59, 'at': 60, 'just': 61, 'or': 62, 'his': 63, 'comedy': 64, 'life': 65, 'surprisingly': 66, 'what': 67, 'like': 68, 'watching': 69, 'old': 70, 'better': 71, 'their': 72, 'every': 73, 'all': 74, 'does': 75, 'hate': 76, \"''\": 77, 'many': 78, 'recent': 79, 'into': 80, 'moments': 81, 'completely': 82, 'work': 83, 'men': 84, 'seems': 85, 'despite': 86, 'amounts': 87, 'will': 88, 'laughs': 89, 'effects': 90, 'make': 91, 'well': 92, 'worth': 93, 'unfortunately': 94, 'sensitive': 95, 'two': 96, 'being': 97, 'again': 98, 'even': 99, 'may': 100, 'feels': 101, 'rock': 102, 'line': 103, 'which': 104, 'was': 105, ':': 106, 'wildly': 107, 'step': 108, 'he': 109, 'suspense': 110, 'tribute': 111, 'considerable': 112, 'aplomb': 113, 'who': 114, 'want': 115, 'serves': 116, 'depth': 117, 'itself': 118, 'talented': 119, 'through': 120, 'has': 121, 'charm': 122, 'once': 123, 'small': 124, 'superb': 125, 'dumb': 126, 'fun': 127, 'poignant': 128, 'new': 129, 'plot': 130, 'whole': 131, 'mess': 132, 'derivative': 133, 'noir': 134, 'crime': 135, 'much': 136, 'romance': 137, 'dialogue': 138, 'only': 139, 'fear': 140, 'digital': 141, 'point': 142, 'than': 143, 'awful': 144, 'part': 145, 'late': 146, 'uses': 147, 'sensational': 148, 'horror': 149, 'seen': 150, 'heart': 151, 'ride': 152, 'epic': 153, 'kind': 154, 'look': 155, 'would': 156, 'flaws': 157, 'attractive': 158, 'watch': 159, 'jason': 160, 'special': 161, 'justify': 162, 'theatrical': 163, 'simulation': 164, 'death': 165, 'camp': 166, 'auschwitz': 167, 'ii-birkenau': 168, 'cinematic': 169, 'history': 170, 'how': 171, 'put': 172, 'after': 173, 'sketchy': 174, 'makes': 175, 'together': 176, 'family': 177, 'silly': 178, 'almost': 179, 'any': 180, 'year': 181, 'enough': 182, 'sometimes': 183, 'beautiful': 184, 'minutes': 185, 'young': 186, 'outrageous': 187, 'out': 188, 'conviction': 189, 'cuteness': 190, 'pay': 191, 'think': 192, 'scene': 193, 'best': 194, 'smooth': 195, 'professional': 196, 'admittedly': 197, 'middling': 198, 'hold': 199, 'grips': 200, 'hard': 201, 'her': 202, 'charmless': 203, 'disappointingly': 204, 'thin': 205, 'slice': 206, 'lower-class': 207, 'london': 208, ';': 209, 'title': 210, 'degraded': 211, 'handheld': 212, 'blair': 213, 'witch': 214, 'video-cam': 215, 'footage': 216, 'home': 217, 'leave': 218, 'wanting': 219, 'mention': 220, 'leaving': 221, 'smile': 222, 'face': 223, 'breathless': 224, 'anticipation': 225, 'setup': 226, 'easy': 227, 'borders': 228, 'facile': 229, 'guns': 230, 'cheatfully': 231, 'filmed': 232, 'martial': 233, 'arts': 234, 'disintegrating': 235, 'bloodsucker': 236, 'computer': 237, 'jagged': 238, 'camera': 239, 'moves': 240, 'valuable': 241, 'messages': 242, 'speak': 243, 'while': 244, 'forces': 245, 'ponder': 246, 'anew': 247, 'résumé': 248, 'loaded': 249, 'credits': 250, 'girl': 251, 'bar': 252, '#': 253, '3': 254, 'retains': 255, 'ambiguities': 256, 'pays': 257, 'earnest': 258, 'homage': 259, 'turntablists': 260, 'beat': 261, 'jugglers': 262, 'schoolers': 263, 'current': 264, 'innovators': 265, 'esther': 266, 'kahn': 267, 'unusual': 268, 'also': 269, 'irritating': 270, 'working': 271, 'script': 272, 'co-written': 273, 'gianni': 274, 'romoli': 275, 'ozpetek': 276, 'avoids': 277, 'pitfalls': 278, \"'d\": 279, 'expect': 280, 'such': 281, 'potentially': 282, 'sudsy': 283, 'set-up': 284, 'used': 285, 'my': 286, 'hours': 287, 'john': 288, 'malkovich': 289, 'rich': 290, 'full': 291, 'kinetic': 292, 'teeming': 293, 'cranky': 294, 'adults': 295, 'rediscover': 296, 'quivering': 297, 'kid': 298, 'inside': 299, 'positive': 300, 'change': 301, 'tone': 302, 'here': 303, 'recharged': 304, 'him': 305, 'bogus': 306, 'ludicrous': 307, 'significantly': 308, 'merit': 309, '103-minute': 310, 'length': 311, 'everything': 312, 'girls': 313, 'ca': 314, 'swim': 315, 'passages': 316, 'observation': 317, 'secondhand': 318, 'familiar': 319, 'fighting': 320, 'skills': 321, 'steven': 322, 'seagal': 323, 'tedious': 324, 'norwegian': 325, 'offering': 326, 'somehow': 327, 'snagged': 328, 'oscar': 329, 'nomination': 330, 'showing': 331, 'honest': 332, 'emotions': 333, 'produced': 334, 'jerry': 335, 'bruckheimer': 336, 'directed': 337, 'joel': 338, 'schumacher': 339, 'reflects': 340, 'shallow': 341, 'styles': 342, 'overproduced': 343, 'inadequately': 344, 'motivated': 345, 'demographically': 346, 'targeted': 347, 'please': 348, 'carnage': 349, 'because': 350, 'acts': 351, 'goofy': 352, 'handsome': 353, 'unfulfilling': 354, 'drama': 355, 'warm': 356, 'water': 357, 'under': 358, 'red': 359, 'bridge': 360, 'celebration': 361, 'feminine': 362, 'energy': 363, 'power': 364, 'women': 365, 'heal': 366, 'confessions': 367, 'straightforward': 368, 'bio': 369, 'unassuming': 370, 'subordinate': 371, 'guys': 372, 'desperately': 373, 'quentin': 374, 'tarantino': 375, 'they': 376, 'grow': 377, 'up': 378, 'amazingly': 379, 'lame': 380, 'usual': 381, 'auto-critique': 382, 'clumsiness': 383, 'damning': 384, 'censure': 385, 'take': 386, 'care': 387, 'nicely': 388, 'performed': 389, 'quintet': 390, 'actresses': 391, 'added': 392, 'resonance': 393, 'folly': 394, 'superficiality': 395, 'entirely': 396, 'persuasive': 397, 'give': 398, 'exposure': 399, 'performers': 400, 'halfway': 401, 'beginning': 402, 'hugh': 403, 'grant': 404, 'dentist': 405, 'waiting': 406, 'room': 407, 'slick': 408, 'manufactured': 409, 'claim': 410, 'street': 411, 'credibility': 412, 'laughable': 413, 'compulsively': 414, 'watchable': 415, 'either': 416, 'extremely': 417, 'unpleasant': 418, 'bewilderingly': 419, 'brilliant': 420, 'entertaining': 421, 'enables': 422, 'shafer': 423, 'navigate': 424, 'spaces': 425, 'both': 426, 'large': 427, 'breezy': 428, 'distracted': 429, 'rhythms': 430, 'vicious': 431, 'absurd': 432, 'price': 433, 'admission': 434, 'gory': 435, 'mayhem': 436, 'idea': 437, 'stylish': 438, 'exercise': 439, 'technically': 440, 'understand': 441, 'difference': 442, 'between': 443, 'plain': 444, 'shot': 445, 'artful': 446, 'watery': 447, 'tones': 448, 'blue': 449, 'green': 450, 'brown': 451, 'leavened': 452, 'conceptions': 453, 'bode': 454, 'rest': 455, 'comic': 456, 'gem': 457, 'delightful': 458, 'affair': 459, 'true': 460, 'incredibly': 461, 'hokey': 462, 'unorthodox': 463, 'organized': 464, 'includes': 465, 'strangest': 466, 'manages': 467, 'infuse': 468, 'rocky': 469, 'path': 470, 'sibling': 471, 'reconciliation': 472, 'flashes': 473, 'warmth': 474, 'gentle': 475, 'humor': 476, 'essentially': 477, 'collection': 478, 'bits': 479, 'frida': 480, 'different': 481, 'hollywood': 482, 'second': 483, 'fiddle': 484, 'ultra-cheesy': 485, 'traditionally': 486, 'structured': 487, 'thing': 488, 'dot': 489, 'com': 490, 'ugly': 491, 'shabby': 492, 'photography': 493, 'add': 494, 'beyond': 495, 'dark': 496, 'visions': 497, 'already': 498, 'relayed': 499, 'predecessors': 500, 'adventurous': 501, 'indian': 502, 'filmmakers': 503, 'toward': 504, 'crossover': 505, 'nonethnic': 506, 'markets': 507, 'chances': 508, 'bold': 509, 'studio': 510, 'standards': 511, 'fascinating': 512, 'byways': 513, 'thoroughly': 514, 'thanks': 515, 'lau': 516, 'arrive': 517, 'early': 518, 'stay': 519, 'evanescent': 520, 'seamless': 521, 'sumptuous': 522, 'stream': 523, 'acted': 524, 'diane': 525, 'lane': 526, 'richard': 527, 'gere': 528, 'real-life': 529, '19th-century': 530, 'metaphor': 531, 'scant': 532, 'dim': 533, 'spare': 534, 'wildlife': 535, 'things': 536, 'we': 537, \"'ve\": 538, 'before': 539, 'pan-american': 540, 'genuine': 541, 'insight': 542, 'urban': 543, 'enjoy': 544, 'value': 545, 'respect': 546, 'term': 547, 'cinema': 548, 'yes': 549, 'snail-like': 550, 'pacing': 551, 'elevated': 552, 'predictable': 553, 'tides': 554, 'smeary': 555, 'blurry': 556, 'distraction': 557, 'inane': 558, 'liked': 559, 'had': 560, 'gone': 561, 'further': 562, 'bring': 563, 'tissues': 564, 'challenges': 565, 'poses': 566, 'forgive': 567, 'classic': 568, 'casts': 569, 'actors': 570, 'magnificent': 571, 'landscape': 572, 'create': 573, 'feature': 574, 'wickedly': 575, 'well-thought': 576, 'stunts': 577, 'compelling': 578, 'lika': 579, 'da': 580, 'believe': 581, 'actually': 582, 'backseat': 583, 'imax': 584, 'short': 585, 'problem': 586, 'whether': 587, 'these': 588, 'ambitions': 589, 'laudable': 590, 'themselves': 591, 'effective': 592, 'stick': 593, 'addition': 594, 'sporting': 595, 'titles': 596, 'trying': 597, 'grab': 598, 'lump': 599, 'play-doh': 600, 'harder': 601, 'liman': 602, 'tries': 603, 'squeeze': 604, 'reveals': 605, 'important': 606, 'our': 607, 'talents': 608, 'service': 609, 'others': 610, 'laugh': 611, 'maybe': 612, 'twice': 613, 'forgotten': 614, 'get': 615, 'back': 616, 'car': 617, 'parking': 618, 'lot': 619, 'trouble': 620, 'day': 621, 'preliminary': 622, 'notes': 623, 'science-fiction': 624, 'fragmentary': 625, 'narrative': 626, 'style': 627, 'piecing': 628, 'frustrating': 629, 'difficult': 630, 'lively': 631, 'engaging': 632, 'examination': 633, 'similar': 634, 'obsessions': 635, 'dominate': 636, 'yet': 637, 'grating': 638, 'showcase': 639, 'overbearing': 640, 'over-the-top': 641, 'direction': 642, 'fluid': 643, 'no-nonsense': 644, 'authority': 645, 'performances': 646, 'harris': 647, 'phifer': 648, 'cam': 649, '`': 650, 'ron': 651, 'seal': 652, 'deal': 653, 'infectiously': 654, 'wide-awake': 655, 'passable': 656, 'date': 657, 'cobbled': 658, 'largely': 659, 'flat': 660, 'uncreative': 661, 'remarkable': 662, 'procession': 663, 'sweeping': 664, 'pictures': 665, 'reinvigorated': 666, 'genre': 667, 'told': 668, 'scattered': 669, 'fashion': 670, 'build': 671, 'robots': 672, 'haul': 673, \"'em\": 674, 'theater': 675, 'show': 676, 'mystery': 677, 'science': 678, 'theatre': 679, '3000': 680, 'certainly': 681, 'going': 682, 'go': 683, 'down': 684, 'killer': 685, 'website': 686, 'other': 687, 'tear': 688, 'eyes': 689, 'away': 690, 'images': 691, 'long': 692, 'read': 693, 'subtitles': 694, 'adaptation': 695, 'provide': 696, 'keenest': 697, 'pleasures': 698, 'extraordinary': 699, 'faith': 700, 'hammily': 701, 'grievous': 702, 'runs': 703, 'mere': 704, '84': 705, 'glance': 706, 'tasteful': 707, 'roll': 708, 'overlong': 709, 'bombastic': 710, 'willing': 711, 'champion': 712, 'fallibility': 713, 'human': 714, 'absolutely': 715, 'ridiculous': 716, 'mid-to-low': 717, 'budget': 718, 'betrayed': 719, 'shoddy': 720, 'makeup': 721, 'woman': 722, 'great': 723, 'generosity': 724, 'diplomacy': 725, 'collapse': 726, 'ingenious': 727, 'delivers': 728, 'promises': 729, 'wild': 730, 'ensues': 731, 'brash': 732, 'set': 733, 'conquer': 734, 'online': 735, 'world': 736, 'laptops': 737, 'cell': 738, 'phones': 739, 'business': 740, 'plans': 741, 'audacious': 742, 'supremely': 743, 'unfunny': 744, 'unentertaining': 745, 'middle-age': 746, 'brings': 747, 'proper': 748, 'role': 749, 'bourne': 750, 'uneven': 751, 'proves': 752, 'lovely': 753, 'trifle': 754, 'love': 755, 'pleasant': 756, 'oozing': 757, 'well-written': 758, 'well-acted': 759, 'easily': 760, 'wait': 761, 'per': 762, 'view': 763, 'dollar': 764, 'workable': 765, 'primer': 766, 'region': 767, 'terrific': 768, '10th-grade': 769, 'learning': 770, 'tool': 771, 'realistic': 772, 'portrayal': 773, 'involved': 774, 'save': 775, 'dash': 776, 'shows': 777, 'slightest': 778, 'aptitude': 779, 'acting': 780, 'pokes': 781, 'provokes': 782, 'expressionistic': 783, 'license': 784, 'vainly': 785, 'hopeless': 786, 'do': 787, 'concert': 788, 'unpretentious': 789, 'charming': 790, 'quirky': 791, 'original': 792, 'clumsy': 793, 'heavy-handed': 794, 'phoney-feeling': 795, 'sentiment': 796, 'caruso': 797, 'descends': 798, 'sub-tarantino': 799, 'sure': 800, 'salton': 801, 'sea': 802, 'works': 803, 'should': 804, 'keeping': 805, 'tight': 806, 'nasty': 807, 'big': 808, 'excuse': 809, 'play': 810, 'lewd': 811, 'another': 812, 'black': 813, 'ii': 814, 'achieves': 815, 'ultimate': 816, 'insignificance': 817, 'sci-fi': 818, 'spectacle': 819, 'whiffle-ball': 820, 'unpredictable': 821, 'digital-effects-heavy': 822, 'supposed': 823, 'family-friendly': 824, 'hawaiian': 825, 'shirt': 826, 'altogether': 827, 'slight': 828, 'called': 829, 'masterpiece': 830, 'nicks': 831, 'steinberg': 832, 'match': 833, 'creations': 834, 'pure': 835, 'venality': 836, 'giving': 837, 'college': 838, 'try': 839, 'able': 840, 'hit': 841, '15-year': 842, \"'re\": 843, 'over': 844, '100': 845, 'starts': 846, 'off': 847, 'bad': 848, 'feel': 849, 'running': 850, 'screaming': 851, 'dog': 852, 'underscore': 853, 'importance': 854, 'tradition': 855, 'familial': 856, 'community': 857, 'conclusive': 858, 'answers': 859, 'character': 860, 'dramas': 861, 'never': 862, 'reach': 863, 'satisfying': 864, 'conclusions': 865, 'gender-bending': 866, 'generally': 867, 'quite': 868, 'funny': 869, 'welcome': 870, 'relief': 871, 'color': 872, 'rather': 873, 'bon': 874, 'bons': 875, 'lost': 876, 'translation': 877, 'lacking': 878, 'surprise': 879, 'consistent': 880, 'emotional': 881, 'promise': 882, 'filmmaking': 883, 'spears': 884, \"'\": 885, 'music': 886, 'videos': 887, 'content': 888, 'except': 889, 'goes': 890, 'least': 891, '90': 892, 'worse': 893, 'see': 894, 'movies': 895, 'suck': 896, 'plodding': 897, 'irritates': 898, 'those': 899, 'so-so': 900, 'films': 901, 'been': 902, 'final': 903, 'eats': 904, 'meddles': 905, 'argues': 906, 'kibbitzes': 907, 'fights': 908, 'elvira': 909, 'fans': 910, 'hardly': 911, 'ask': 912, 'interesting': 913, 'there': 914, 'something': 915, 'artist': 916, '90-plus': 917, 'years': 918, 'taking': 919, 'effort': 920, 'share': 921, 'impressions': 922, 'loss': 923, 'art': 924, 'us': 925, 'respectable': 926, 'disney': 927, 'ransacks': 928, 'archives': 929, 'quick-buck': 930, 'sequel': 931, 'harsh': 932, 'piece': 933, 'storytelling': 934}\n",
            "935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def encode_sentence(sentence):\n",
        "    max_length = 50\n",
        "    input_ids = []\n",
        "    for item in sentence.split():\n",
        "      if item in vocab_to_int:\n",
        "        input_ids.append(vocab_to_int[item])\n",
        "      else:\n",
        "        input_ids.append(vocab_to_int['<unk>'])\n",
        "\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids += [vocab_to_int['<pad>']] * padding_length\n",
        "    return np.array(input_ids)\n",
        "\n",
        "def encode_label(label):\n",
        "    return np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Train Dataset Encode Examples\")\n",
        "for a,b in zip(train_texts[:3],train_labels[:3]):\n",
        "  print(encode_sentence(a),b)"
      ],
      "metadata": {
        "id": "GJ0RsEBgqidq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6552e44b-984d-40c6-a8a1-227afa5579d0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Encode Examples\n",
            "[  8  12 195   3 196   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n",
            "[ 37 197 198  15   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n",
            "[ 38 199   3 200 201   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.embedding_weights = np.random.rand(vocab_size, embedding_dim) # Embedding weights\n",
        "        limit = np.sqrt(6 / (embedding_dim + hidden_size))\n",
        "        self.W1 = np.random.uniform(-limit, limit, size=(embedding_dim, hidden_size))\n",
        "        limit = np.sqrt(6/ (hidden_size+output_size))\n",
        "        self.W2 = np.random.uniform(-limit, limit, size=(hidden_size, output_size))\n",
        "        # self.W1 = np.random.rand(embedding_dim, hidden_size)   # Input to hidden weights\n",
        "        # self.W2 = np.random.rand(hidden_size, output_size) # Hidden to output weights\n",
        "        self.b1 = np.zeros((1, hidden_size))  # Hidden layer biases\n",
        "        self.b2 = np.zeros((1, output_size))  # Output layer biases\n",
        "        self.X = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass\n",
        "        embedded_x = self.embedding_weights[X]\n",
        "        self.sentence_vec = np.mean(embedded_x, axis=0, keepdims=True)\n",
        "\n",
        "        z1 = np.dot(self.sentence_vec, self.W1) + self.b1\n",
        "        self.h = self.sigmoid(z1)\n",
        "\n",
        "        z2 = np.dot(self.h, self.W2) + self.b2\n",
        "        output = self.sigmoid(z2)\n",
        "        return output\n",
        "\n",
        "    def compute_loss(self, y, output):\n",
        "        # Compute binary cross-entropy loss\n",
        "        return -np.sum(y * np.log(output) + (1 - y) * np.log(1 - output))\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.01):\n",
        "      delta2 = output - y\n",
        "      grad_W2 = np.dot(self.h.T, delta2)\n",
        "      grad_b2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "      delta1 = np.dot(delta2, self.W2.T) * self.sigmoid_derivative(self.h)\n",
        "      grad_W1 = np.dot(self.sentence_vec.T, delta1)\n",
        "      grad_b1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "      self.W1 -= learning_rate * grad_W1\n",
        "      self.b1 -= learning_rate * grad_b1\n",
        "      self.W2 -= learning_rate * grad_W2\n",
        "      self.b2 -= learning_rate * grad_b2"
      ],
      "metadata": {
        "id": "-76p4FBbAqLp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def train(X, y, learning_rate=0.01):\n",
        "    output = nn.forward(X)  # Forward pass\n",
        "    loss = nn.compute_loss(y, output)  # Compute loss\n",
        "    nn.backward(X, y, output, learning_rate)  # Backward pass\n",
        "    return loss\n",
        "\n",
        "def predict(x):\n",
        "    output = nn.forward(x)\n",
        "    return output, (output > 0.5).astype(int)  # Binary classification\n",
        "\n",
        "# Initialize the neural network\n",
        "vocab_size = len(vocab_to_int)  # Number of unique words in vocab\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "hidden_size = 80  # Number of neurons in the hidden layer\n",
        "output_size = 1  # One output (binary classification)\n",
        "learning_rate = 0.01\n",
        "nn = SimpleNN(vocab_size, embedding_dim, hidden_size, output_size)\n",
        "\n",
        "true, pred = [], []\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    true, pred = [], []  # Reset true and predicted labels for each epoch\n",
        "    for x, y in zip(train_texts, train_labels):\n",
        "        x = encode_sentence(x)  # Encode sentence as word indices\n",
        "        train_loss += train(x, y, learning_rate)  # Train on the current sample\n",
        "        _, prediction = predict(x)  # obtain prediction\n",
        "        true.append(y)  # Append true label\n",
        "        pred.append(prediction[0][0])  # Append predicted label (extract scalar)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_acc = accuracy_score(true, pred) * 100.0\n",
        "    train_loss /= len(train_texts)  # Average training loss\n",
        "\n",
        "    # Evaluate on dev set\n",
        "    dev_true, dev_pred = [], []\n",
        "    dev_loss = 0.0\n",
        "    for x, y in zip(dev_texts, dev_labels):\n",
        "        x = encode_sentence(x)\n",
        "\n",
        "        output, prediction = predict(x)\n",
        "        dev_loss += nn.compute_loss(y,output)\n",
        "        dev_true.append(y)\n",
        "        dev_pred.append(prediction[0][0])\n",
        "    dev_loss /= len(dev_texts)\n",
        "    dev_acc = accuracy_score(dev_true, dev_pred) * 100.0\n",
        "    print(f'{epoch} epoch, train_loss = {train_loss:.4f}, train_acc: {train_acc:.2f}%, eval_loss: {dev_loss:.4f}, eval_acc: {dev_acc:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBBBnxduXd_u",
        "outputId": "32d80404-edd1-4511-e811-2c84ed68fe68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 epoch, train_loss = 0.7216, train_acc: 69.00%, eval_loss: 0.6941, eval_acc: 50.00%\n",
            "1 epoch, train_loss = 0.7205, train_acc: 66.00%, eval_loss: 0.6942, eval_acc: 50.00%\n",
            "2 epoch, train_loss = 0.7203, train_acc: 66.00%, eval_loss: 0.6942, eval_acc: 50.00%\n",
            "3 epoch, train_loss = 0.7202, train_acc: 66.00%, eval_loss: 0.6943, eval_acc: 50.00%\n",
            "4 epoch, train_loss = 0.7201, train_acc: 66.00%, eval_loss: 0.6943, eval_acc: 50.00%\n",
            "5 epoch, train_loss = 0.7199, train_acc: 66.00%, eval_loss: 0.6944, eval_acc: 50.00%\n",
            "6 epoch, train_loss = 0.7198, train_acc: 66.00%, eval_loss: 0.6944, eval_acc: 50.00%\n",
            "7 epoch, train_loss = 0.7196, train_acc: 66.00%, eval_loss: 0.6944, eval_acc: 50.00%\n",
            "8 epoch, train_loss = 0.7195, train_acc: 66.00%, eval_loss: 0.6945, eval_acc: 50.00%\n",
            "9 epoch, train_loss = 0.7193, train_acc: 66.00%, eval_loss: 0.6945, eval_acc: 50.00%\n",
            "10 epoch, train_loss = 0.7192, train_acc: 66.00%, eval_loss: 0.6946, eval_acc: 50.00%\n",
            "11 epoch, train_loss = 0.7191, train_acc: 66.00%, eval_loss: 0.6946, eval_acc: 50.00%\n",
            "12 epoch, train_loss = 0.7189, train_acc: 66.00%, eval_loss: 0.6947, eval_acc: 50.00%\n",
            "13 epoch, train_loss = 0.7188, train_acc: 66.00%, eval_loss: 0.6947, eval_acc: 50.00%\n",
            "14 epoch, train_loss = 0.7186, train_acc: 66.00%, eval_loss: 0.6948, eval_acc: 50.00%\n",
            "15 epoch, train_loss = 0.7185, train_acc: 66.00%, eval_loss: 0.6948, eval_acc: 50.00%\n",
            "16 epoch, train_loss = 0.7184, train_acc: 66.00%, eval_loss: 0.6949, eval_acc: 50.00%\n",
            "17 epoch, train_loss = 0.7182, train_acc: 66.00%, eval_loss: 0.6949, eval_acc: 50.00%\n",
            "18 epoch, train_loss = 0.7181, train_acc: 66.00%, eval_loss: 0.6950, eval_acc: 50.00%\n",
            "19 epoch, train_loss = 0.7180, train_acc: 66.00%, eval_loss: 0.6950, eval_acc: 50.00%\n",
            "20 epoch, train_loss = 0.7178, train_acc: 66.00%, eval_loss: 0.6950, eval_acc: 50.00%\n",
            "21 epoch, train_loss = 0.7177, train_acc: 66.00%, eval_loss: 0.6951, eval_acc: 50.00%\n",
            "22 epoch, train_loss = 0.7176, train_acc: 66.00%, eval_loss: 0.6951, eval_acc: 50.00%\n",
            "23 epoch, train_loss = 0.7175, train_acc: 66.00%, eval_loss: 0.6952, eval_acc: 50.00%\n",
            "24 epoch, train_loss = 0.7173, train_acc: 66.00%, eval_loss: 0.6952, eval_acc: 50.00%\n",
            "25 epoch, train_loss = 0.7172, train_acc: 65.00%, eval_loss: 0.6953, eval_acc: 50.00%\n",
            "26 epoch, train_loss = 0.7171, train_acc: 65.00%, eval_loss: 0.6953, eval_acc: 50.00%\n",
            "27 epoch, train_loss = 0.7169, train_acc: 65.00%, eval_loss: 0.6953, eval_acc: 50.00%\n",
            "28 epoch, train_loss = 0.7168, train_acc: 65.00%, eval_loss: 0.6954, eval_acc: 50.00%\n",
            "29 epoch, train_loss = 0.7167, train_acc: 65.00%, eval_loss: 0.6954, eval_acc: 50.00%\n",
            "30 epoch, train_loss = 0.7166, train_acc: 65.00%, eval_loss: 0.6955, eval_acc: 50.00%\n",
            "31 epoch, train_loss = 0.7164, train_acc: 65.00%, eval_loss: 0.6955, eval_acc: 50.00%\n",
            "32 epoch, train_loss = 0.7163, train_acc: 65.00%, eval_loss: 0.6956, eval_acc: 50.00%\n",
            "33 epoch, train_loss = 0.7162, train_acc: 65.00%, eval_loss: 0.6956, eval_acc: 50.00%\n",
            "34 epoch, train_loss = 0.7161, train_acc: 65.00%, eval_loss: 0.6956, eval_acc: 50.00%\n",
            "35 epoch, train_loss = 0.7159, train_acc: 65.00%, eval_loss: 0.6957, eval_acc: 50.00%\n",
            "36 epoch, train_loss = 0.7158, train_acc: 66.00%, eval_loss: 0.6957, eval_acc: 50.00%\n",
            "37 epoch, train_loss = 0.7157, train_acc: 66.00%, eval_loss: 0.6958, eval_acc: 50.00%\n",
            "38 epoch, train_loss = 0.7156, train_acc: 66.00%, eval_loss: 0.6958, eval_acc: 50.00%\n",
            "39 epoch, train_loss = 0.7155, train_acc: 65.00%, eval_loss: 0.6958, eval_acc: 50.00%\n",
            "40 epoch, train_loss = 0.7153, train_acc: 65.00%, eval_loss: 0.6959, eval_acc: 50.00%\n",
            "41 epoch, train_loss = 0.7152, train_acc: 65.00%, eval_loss: 0.6959, eval_acc: 50.00%\n",
            "42 epoch, train_loss = 0.7151, train_acc: 65.00%, eval_loss: 0.6959, eval_acc: 50.00%\n",
            "43 epoch, train_loss = 0.7150, train_acc: 65.00%, eval_loss: 0.6960, eval_acc: 50.00%\n",
            "44 epoch, train_loss = 0.7149, train_acc: 66.00%, eval_loss: 0.6960, eval_acc: 50.00%\n",
            "45 epoch, train_loss = 0.7148, train_acc: 66.00%, eval_loss: 0.6961, eval_acc: 50.00%\n",
            "46 epoch, train_loss = 0.7147, train_acc: 66.00%, eval_loss: 0.6961, eval_acc: 50.00%\n",
            "47 epoch, train_loss = 0.7145, train_acc: 66.00%, eval_loss: 0.6961, eval_acc: 50.00%\n",
            "48 epoch, train_loss = 0.7144, train_acc: 67.00%, eval_loss: 0.6962, eval_acc: 50.00%\n",
            "49 epoch, train_loss = 0.7143, train_acc: 67.00%, eval_loss: 0.6962, eval_acc: 50.00%\n",
            "50 epoch, train_loss = 0.7142, train_acc: 67.00%, eval_loss: 0.6962, eval_acc: 50.00%\n",
            "51 epoch, train_loss = 0.7141, train_acc: 67.00%, eval_loss: 0.6963, eval_acc: 50.00%\n",
            "52 epoch, train_loss = 0.7140, train_acc: 67.00%, eval_loss: 0.6963, eval_acc: 50.00%\n",
            "53 epoch, train_loss = 0.7139, train_acc: 67.00%, eval_loss: 0.6963, eval_acc: 50.00%\n",
            "54 epoch, train_loss = 0.7138, train_acc: 67.00%, eval_loss: 0.6964, eval_acc: 50.00%\n",
            "55 epoch, train_loss = 0.7136, train_acc: 67.00%, eval_loss: 0.6964, eval_acc: 50.00%\n",
            "56 epoch, train_loss = 0.7135, train_acc: 67.00%, eval_loss: 0.6964, eval_acc: 50.00%\n",
            "57 epoch, train_loss = 0.7134, train_acc: 67.00%, eval_loss: 0.6965, eval_acc: 50.00%\n",
            "58 epoch, train_loss = 0.7133, train_acc: 67.00%, eval_loss: 0.6965, eval_acc: 50.00%\n",
            "59 epoch, train_loss = 0.7132, train_acc: 67.00%, eval_loss: 0.6966, eval_acc: 50.00%\n",
            "60 epoch, train_loss = 0.7131, train_acc: 67.00%, eval_loss: 0.6966, eval_acc: 50.00%\n",
            "61 epoch, train_loss = 0.7130, train_acc: 67.00%, eval_loss: 0.6966, eval_acc: 50.00%\n",
            "62 epoch, train_loss = 0.7129, train_acc: 66.00%, eval_loss: 0.6967, eval_acc: 50.00%\n",
            "63 epoch, train_loss = 0.7128, train_acc: 66.00%, eval_loss: 0.6967, eval_acc: 50.00%\n",
            "64 epoch, train_loss = 0.7127, train_acc: 65.00%, eval_loss: 0.6967, eval_acc: 50.00%\n",
            "65 epoch, train_loss = 0.7126, train_acc: 65.00%, eval_loss: 0.6967, eval_acc: 50.00%\n",
            "66 epoch, train_loss = 0.7125, train_acc: 65.00%, eval_loss: 0.6968, eval_acc: 50.00%\n",
            "67 epoch, train_loss = 0.7124, train_acc: 65.00%, eval_loss: 0.6968, eval_acc: 50.00%\n",
            "68 epoch, train_loss = 0.7123, train_acc: 65.00%, eval_loss: 0.6968, eval_acc: 50.00%\n",
            "69 epoch, train_loss = 0.7122, train_acc: 65.00%, eval_loss: 0.6969, eval_acc: 50.00%\n",
            "70 epoch, train_loss = 0.7121, train_acc: 65.00%, eval_loss: 0.6969, eval_acc: 50.00%\n",
            "71 epoch, train_loss = 0.7120, train_acc: 65.00%, eval_loss: 0.6969, eval_acc: 50.00%\n",
            "72 epoch, train_loss = 0.7119, train_acc: 65.00%, eval_loss: 0.6970, eval_acc: 50.00%\n",
            "73 epoch, train_loss = 0.7118, train_acc: 65.00%, eval_loss: 0.6970, eval_acc: 50.00%\n",
            "74 epoch, train_loss = 0.7117, train_acc: 65.00%, eval_loss: 0.6970, eval_acc: 50.00%\n",
            "75 epoch, train_loss = 0.7116, train_acc: 65.00%, eval_loss: 0.6971, eval_acc: 50.00%\n",
            "76 epoch, train_loss = 0.7115, train_acc: 65.00%, eval_loss: 0.6971, eval_acc: 50.00%\n",
            "77 epoch, train_loss = 0.7114, train_acc: 65.00%, eval_loss: 0.6971, eval_acc: 50.00%\n",
            "78 epoch, train_loss = 0.7113, train_acc: 65.00%, eval_loss: 0.6971, eval_acc: 50.00%\n",
            "79 epoch, train_loss = 0.7112, train_acc: 65.00%, eval_loss: 0.6972, eval_acc: 50.00%\n",
            "80 epoch, train_loss = 0.7111, train_acc: 65.00%, eval_loss: 0.6972, eval_acc: 50.00%\n",
            "81 epoch, train_loss = 0.7110, train_acc: 65.00%, eval_loss: 0.6972, eval_acc: 50.00%\n",
            "82 epoch, train_loss = 0.7109, train_acc: 65.00%, eval_loss: 0.6973, eval_acc: 50.00%\n",
            "83 epoch, train_loss = 0.7108, train_acc: 65.00%, eval_loss: 0.6973, eval_acc: 50.00%\n",
            "84 epoch, train_loss = 0.7107, train_acc: 65.00%, eval_loss: 0.6973, eval_acc: 50.00%\n",
            "85 epoch, train_loss = 0.7106, train_acc: 65.00%, eval_loss: 0.6973, eval_acc: 50.00%\n",
            "86 epoch, train_loss = 0.7105, train_acc: 65.00%, eval_loss: 0.6974, eval_acc: 50.00%\n",
            "87 epoch, train_loss = 0.7105, train_acc: 65.00%, eval_loss: 0.6974, eval_acc: 50.00%\n",
            "88 epoch, train_loss = 0.7104, train_acc: 65.00%, eval_loss: 0.6974, eval_acc: 50.00%\n",
            "89 epoch, train_loss = 0.7103, train_acc: 65.00%, eval_loss: 0.6974, eval_acc: 50.00%\n",
            "90 epoch, train_loss = 0.7102, train_acc: 65.00%, eval_loss: 0.6975, eval_acc: 50.00%\n",
            "91 epoch, train_loss = 0.7101, train_acc: 65.00%, eval_loss: 0.6975, eval_acc: 50.00%\n",
            "92 epoch, train_loss = 0.7100, train_acc: 65.00%, eval_loss: 0.6975, eval_acc: 50.00%\n",
            "93 epoch, train_loss = 0.7099, train_acc: 65.00%, eval_loss: 0.6976, eval_acc: 50.00%\n",
            "94 epoch, train_loss = 0.7098, train_acc: 65.00%, eval_loss: 0.6976, eval_acc: 50.00%\n",
            "95 epoch, train_loss = 0.7097, train_acc: 65.00%, eval_loss: 0.6976, eval_acc: 50.00%\n",
            "96 epoch, train_loss = 0.7097, train_acc: 65.00%, eval_loss: 0.6976, eval_acc: 51.00%\n",
            "97 epoch, train_loss = 0.7096, train_acc: 65.00%, eval_loss: 0.6977, eval_acc: 50.00%\n",
            "98 epoch, train_loss = 0.7095, train_acc: 65.00%, eval_loss: 0.6977, eval_acc: 50.00%\n",
            "99 epoch, train_loss = 0.7094, train_acc: 65.00%, eval_loss: 0.6977, eval_acc: 50.00%\n"
          ]
        }
      ]
    }
  ]
}